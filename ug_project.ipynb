{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e154a47bf09b8770980486e87786317a1b3038e1"
   },
   "source": [
    "### Meeting a Sayed Athar's request, I'm using the Kernel altered by Khoi Nguyen to explain how the whole code works.\n",
    "### If any part is not clear, please comment.  \n",
    "### Please upvote if it was helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq # Used to read the data\n",
    "import os \n",
    "import numpy as np\n",
    "from keras.layers import * # Keras is the most friendly Neural Network library, this Kernel use a lot of layers classes\n",
    "from keras.models import Model\n",
    "from tqdm import tqdm # Processing time measurement\n",
    "from sklearn.model_selection import train_test_split \n",
    "from keras import backend as K # The backend give us access to tensorflow operations and allow us to create the Attention class\n",
    "from keras import optimizers # Allow us to access the Adam class to modify some parameters\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold # Used to use Kfold to train our model\n",
    "from keras.callbacks import * # This object helps the model to train in a smarter way, avoiding overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "6e6379386e44afc69bee8895a52da22199e888fb"
   },
   "outputs": [],
   "source": [
    "# select how many folds will be created\n",
    "N_SPLITS = 5\n",
    "# it is just a constant with the measurements data size\n",
    "sample_size = 800000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "c3340ee96becb5ca8f075d9c44b7df383ddba5ee"
   },
   "outputs": [],
   "source": [
    "# It is the official metric used in this competition\n",
    "# below is the declaration of a function used inside the keras model, calculation with K (keras backend / thensorflow)\n",
    "def matthews_correlation(y_true, y_pred):\n",
    "    '''Calculates the Matthews correlation coefficient measure for quality\n",
    "    of binary classification problems.\n",
    "    '''\n",
    "    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n",
    "    y_pred_neg = 1 - y_pred_pos\n",
    "\n",
    "    y_pos = K.round(K.clip(y_true, 0, 1))\n",
    "    y_neg = 1 - y_pos\n",
    "\n",
    "    tp = K.sum(y_pos * y_pred_pos)\n",
    "    tn = K.sum(y_neg * y_pred_neg)\n",
    "\n",
    "    fp = K.sum(y_neg * y_pred_pos)\n",
    "    fn = K.sum(y_pos * y_pred_neg)\n",
    "\n",
    "    numerator = (tp * tn - fp * fn)\n",
    "    denominator = K.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "\n",
    "    return numerator / (denominator + K.epsilon())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "eda7ea366117d1ce8e5fce69e5bba333821d8b48"
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/suicaokhoailang/lstm-attention-baseline-0-652-lb\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>signal_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_measurement</th>\n",
       "      <th>phase</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">0</th>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      signal_id  target\n",
       "id_measurement phase                   \n",
       "0              0              0       0\n",
       "               1              1       0\n",
       "               2              2       0\n",
       "1              0              3       1\n",
       "               1              4       1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just load train data\n",
    "df_train = pd.read_csv('../input/metadata_train.csv')\n",
    "# set index, it makes the data access much faster\n",
    "df_train = df_train.set_index(['id_measurement', 'phase'])\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "26df6c7fbfecd537404866faec13d1238ae3ebc6"
   },
   "outputs": [],
   "source": [
    "# in other notebook I have extracted the min and max values from the train data, the measurements\n",
    "max_num = 127\n",
    "min_num = -128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "7b0717b14bcfcba1f48d33c8161ae51c778687af"
   },
   "outputs": [],
   "source": [
    "# This function standardize the data from (-128 to 127) to (-1 to 1)\n",
    "# Theoretically it helps in the NN Model training, but I didn't tested without it\n",
    "def min_max_transf(ts, min_data, max_data, range_needed=(-1,1)):\n",
    "    if min_data < 0:\n",
    "        ts_std = (ts + abs(min_data)) / (max_data + abs(min_data))\n",
    "    else:\n",
    "        ts_std = (ts - min_data) / (max_data - min_data)\n",
    "    if range_needed[0] < 0:    \n",
    "        return ts_std * (range_needed[1] + abs(range_needed[0])) + range_needed[0]\n",
    "    else:\n",
    "        return ts_std * (range_needed[1] - range_needed[0]) + range_needed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "c6137bbbe75c3a1509a5f98e08805dbbd492aa37"
   },
   "outputs": [],
   "source": [
    "# This is one of the most important peace of code of this Kernel\n",
    "# Any power line contain 3 phases of 800000 measurements, or 2.4 millions data \n",
    "# It would be praticaly impossible to build a NN with an input of that size\n",
    "# The ideia here is to reduce it each phase to a matrix of <n_dim> bins by n features\n",
    "# Each bean is a set of 5000 measurements (800000 / 160), so the features are extracted from this 5000 chunk data.\n",
    "def transform_ts(ts, n_dim=160, min_max=(-1,1)):\n",
    "    # convert data into -1 to 1\n",
    "    ts_std = min_max_transf(ts, min_data=min_num, max_data=max_num)\n",
    "    # bucket or chunk size, 5000 in this case (800000 / 160)\n",
    "    bucket_size = int(sample_size / n_dim)\n",
    "    # new_ts will be the container of the new data\n",
    "    new_ts = []\n",
    "    # this for iteract any chunk/bucket until reach the whole sample_size (800000)\n",
    "    for i in range(0, sample_size, bucket_size):\n",
    "        # cut each bucket to ts_range\n",
    "        ts_range = ts_std[i:i + bucket_size]\n",
    "        # calculate each feature\n",
    "        mean = ts_range.mean()\n",
    "        std = ts_range.std() # standard deviation\n",
    "        std_top = mean + std # I have to test it more, but is is like a band\n",
    "        std_bot = mean - std\n",
    "        # I think that the percentiles are very important, it is like a distribuiton analysis from eath chunk\n",
    "        percentil_calc = np.percentile(ts_range, [0, 1, 25, 50, 75, 99, 100]) \n",
    "        max_range = percentil_calc[-1] - percentil_calc[0] # this is the amplitude of the chunk\n",
    "        relative_percentile = percentil_calc - mean # maybe it could heap to understand the asymmetry\n",
    "        # now, we just add all the features to new_ts and convert it to np.array\n",
    "        new_ts.append(np.concatenate([np.asarray([mean, std, std_top, std_bot, max_range]),percentil_calc, relative_percentile]))\n",
    "    return np.asarray(new_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "7460e718a605803f1d9e4fbec61750a0deb02a47"
   },
   "outputs": [],
   "source": [
    "# this function take a piece of data and convert using transform_ts(), but it does to each of the 3 phases\n",
    "# if we would try to do in one time, could exceed the RAM Memmory\n",
    "def prep_data(start, end):\n",
    "    # load a piece of data from file\n",
    "    praq_train = pq.read_pandas('../input/train.parquet', columns=[str(i) for i in range(start, end)]).to_pandas()\n",
    "    X = []\n",
    "    y = []\n",
    "    # using tdqm to evaluate processing time\n",
    "    # takes each index from df_train and iteract it from start to end\n",
    "    # it is divided by 3 because for each id_measurement there are 3 id_signal, and the start/end parameters are id_signal\n",
    "    for id_measurement in tqdm(df_train.index.levels[0].unique()[int(start/3):int(end/3)]):\n",
    "        X_signal = []\n",
    "        # for each phase of the signal\n",
    "        for phase in [0,1,2]:\n",
    "            # extract from df_train both signal_id and target to compose the new data sets\n",
    "            signal_id, target = df_train.loc[id_measurement].loc[phase]\n",
    "            # but just append the target one time, to not triplicate it\n",
    "            if phase == 0:\n",
    "                y.append(target)\n",
    "            # extract and transform data into sets of features\n",
    "            X_signal.append(transform_ts(praq_train[str(signal_id)]))\n",
    "        # concatenate all the 3 phases in one matrix\n",
    "        X_signal = np.concatenate(X_signal, axis=1)\n",
    "        # add the data to X\n",
    "        X.append(X_signal)\n",
    "    X = np.asarray(X)\n",
    "    y = np.asarray(y)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "52dc826ab9ee1dd56c9fb29bd5c1b2d26b5928bf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1452/1452 [08:28<00:00,  3.09it/s]\n",
      "100%|██████████| 1452/1452 [08:22<00:00,  2.36it/s]\n"
     ]
    }
   ],
   "source": [
    "# this code is very simple, divide the total size of the df_train into two sets and process it\n",
    "X = []\n",
    "y = []\n",
    "def load_all():\n",
    "    total_size = len(df_train)\n",
    "    for ini, end in [(0, int(total_size/2)), (int(total_size/2), total_size)]:\n",
    "        X_temp, y_temp = prep_data(ini, end)\n",
    "        X.append(X_temp)\n",
    "        y.append(y_temp)\n",
    "load_all()\n",
    "X = np.concatenate(X)\n",
    "y = np.concatenate(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "51ad0e25b00536de6170168499923d82ae1d735f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2904, 160, 57) (2904,)\n"
     ]
    }
   ],
   "source": [
    "# The X shape here is very important. It is also important undertand a little how a LSTM works\n",
    "# X.shape[0] is the number of id_measuremts contained in train data\n",
    "# X.shape[1] is the number of chunks resultant of the transformation, each of this date enters in the LSTM serialized\n",
    "# This way the LSTM can understand the position of a data relative with other and activate a signal that needs\n",
    "# a serie of inputs in a specifc order.\n",
    "# X.shape[3] is the number of features multiplied by the number of phases (3)\n",
    "print(X.shape, y.shape)\n",
    "# save data into file, a numpy specific format\n",
    "np.save(\"X.npy\",X)\n",
    "np.save(\"y.npy\",y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "289bc7d1ab8048a60025801b457f8df1d848acbc"
   },
   "outputs": [],
   "source": [
    "# This is NN LSTM Model creation\n",
    "def model_lstm(input_shape):\n",
    "    # The shape was explained above, must have this order\n",
    "    inp = Input(shape=(input_shape[1], input_shape[2],))\n",
    "    # This is the LSTM layer\n",
    "    # Bidirecional implies that the 160 chunks are calculated in both ways, 0 to 159 and 159 to zero\n",
    "    # although it appear that just 0 to 159 way matter, I have tested with and without, and tha later worked best\n",
    "    # 128 and 64 are the number of cells used, too many can overfit and too few can underfit\n",
    "    x = Bidirectional(CuDNNLSTM(128, return_sequences=True))(inp)\n",
    "    # The second LSTM can give more fire power to the model, but can overfit it too\n",
    "    x = Bidirectional(CuDNNLSTM(64, return_sequences=True))(x)\n",
    "    # Attention is a new tecnology that can be applyed to a Recurrent NN to give more meanings to a signal found in the middle\n",
    "    # of the data, it helps more in longs chains of data. A normal RNN give all the responsibility of detect the signal\n",
    "    # to the last cell. Google RNN Attention for more information :)\n",
    "    x = Attention(input_shape[1])(x)\n",
    "    # A intermediate full connected (Dense) can help to deal with nonlinears outputs\n",
    "    x = Dense(64, activation=\"relu\")(x)\n",
    "    # A binnary classification as this must finish with shape (1,)\n",
    "    x = Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    # Pay attention in the addition of matthews_correlation metric in the compilation, it is a success factor key\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[matthews_correlation])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "8d6f4ca319c383b1b4f671a37c5a324136e7a466"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning fold 1\n",
      "Train on 2322 samples, validate on 582 samples\n",
      "Epoch 1/50\n",
      "2322/2322 [==============================] - 7s 3ms/step - loss: 0.3796 - matthews_correlation: 0.0000e+00 - val_loss: 0.2420 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_matthews_correlation improved from -inf to 0.00000, saving model to weights_0.h5\n",
      "Epoch 2/50\n",
      "2322/2322 [==============================] - 1s 504us/step - loss: 0.2282 - matthews_correlation: 0.0000e+00 - val_loss: 0.2324 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00002: val_matthews_correlation did not improve from 0.00000\n",
      "Epoch 3/50\n",
      "2322/2322 [==============================] - 1s 504us/step - loss: 0.2216 - matthews_correlation: 0.0000e+00 - val_loss: 0.2176 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00003: val_matthews_correlation did not improve from 0.00000\n",
      "Epoch 4/50\n",
      "2322/2322 [==============================] - 1s 502us/step - loss: 0.2053 - matthews_correlation: 0.0000e+00 - val_loss: 0.2149 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00004: val_matthews_correlation did not improve from 0.00000\n",
      "Epoch 5/50\n",
      "2322/2322 [==============================] - 1s 501us/step - loss: 0.2204 - matthews_correlation: 0.0000e+00 - val_loss: 0.2138 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00005: val_matthews_correlation did not improve from 0.00000\n",
      "Epoch 6/50\n",
      "2322/2322 [==============================] - 1s 504us/step - loss: 0.2024 - matthews_correlation: 0.1099 - val_loss: 0.1989 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00006: val_matthews_correlation did not improve from 0.00000\n",
      "Epoch 7/50\n",
      "2322/2322 [==============================] - 1s 503us/step - loss: 0.1961 - matthews_correlation: 0.0283 - val_loss: 0.1961 - val_matthews_correlation: 0.1923\n",
      "\n",
      "Epoch 00007: val_matthews_correlation improved from 0.00000 to 0.19231, saving model to weights_0.h5\n",
      "Epoch 8/50\n",
      "2322/2322 [==============================] - 1s 511us/step - loss: 0.1750 - matthews_correlation: 0.2789 - val_loss: 0.1521 - val_matthews_correlation: 0.1923\n",
      "\n",
      "Epoch 00008: val_matthews_correlation did not improve from 0.19231\n",
      "Epoch 9/50\n",
      "2322/2322 [==============================] - 1s 502us/step - loss: 0.1609 - matthews_correlation: 0.3023 - val_loss: 0.1219 - val_matthews_correlation: 0.1923\n",
      "\n",
      "Epoch 00009: val_matthews_correlation did not improve from 0.19231\n",
      "Epoch 10/50\n",
      "2322/2322 [==============================] - 1s 527us/step - loss: 0.1312 - matthews_correlation: 0.5377 - val_loss: 0.1199 - val_matthews_correlation: 0.6156\n",
      "\n",
      "Epoch 00010: val_matthews_correlation improved from 0.19231 to 0.61564, saving model to weights_0.h5\n",
      "Epoch 11/50\n",
      "2322/2322 [==============================] - 1s 510us/step - loss: 0.1593 - matthews_correlation: 0.3058 - val_loss: 0.1136 - val_matthews_correlation: 0.1930\n",
      "\n",
      "Epoch 00011: val_matthews_correlation did not improve from 0.61564\n",
      "Epoch 12/50\n",
      "2322/2322 [==============================] - 1s 509us/step - loss: 0.1179 - matthews_correlation: 0.4038 - val_loss: 0.0981 - val_matthews_correlation: 0.7542\n",
      "\n",
      "Epoch 00012: val_matthews_correlation improved from 0.61564 to 0.75422, saving model to weights_0.h5\n",
      "Epoch 13/50\n",
      "2322/2322 [==============================] - 1s 505us/step - loss: 0.1183 - matthews_correlation: 0.5764 - val_loss: 0.0881 - val_matthews_correlation: 0.7550\n",
      "\n",
      "Epoch 00013: val_matthews_correlation improved from 0.75422 to 0.75496, saving model to weights_0.h5\n",
      "Epoch 14/50\n",
      "2322/2322 [==============================] - 1s 505us/step - loss: 0.1195 - matthews_correlation: 0.6233 - val_loss: 0.0965 - val_matthews_correlation: 0.6551\n",
      "\n",
      "Epoch 00014: val_matthews_correlation did not improve from 0.75496\n",
      "Epoch 15/50\n",
      "2322/2322 [==============================] - 1s 500us/step - loss: 0.1114 - matthews_correlation: 0.5789 - val_loss: 0.0856 - val_matthews_correlation: 0.7660\n",
      "\n",
      "Epoch 00015: val_matthews_correlation improved from 0.75496 to 0.76598, saving model to weights_0.h5\n",
      "Epoch 16/50\n",
      "2322/2322 [==============================] - 1s 504us/step - loss: 0.1091 - matthews_correlation: 0.5918 - val_loss: 0.1017 - val_matthews_correlation: 0.7339\n",
      "\n",
      "Epoch 00016: val_matthews_correlation did not improve from 0.76598\n",
      "Epoch 17/50\n",
      "2322/2322 [==============================] - 1s 518us/step - loss: 0.1049 - matthews_correlation: 0.6261 - val_loss: 0.0890 - val_matthews_correlation: 0.7488\n",
      "\n",
      "Epoch 00017: val_matthews_correlation did not improve from 0.76598\n",
      "Epoch 18/50\n",
      "2322/2322 [==============================] - 1s 501us/step - loss: 0.1077 - matthews_correlation: 0.6011 - val_loss: 0.0778 - val_matthews_correlation: 0.7629\n",
      "\n",
      "Epoch 00018: val_matthews_correlation did not improve from 0.76598\n",
      "Epoch 19/50\n",
      "2322/2322 [==============================] - 1s 504us/step - loss: 0.1086 - matthews_correlation: 0.6218 - val_loss: 0.0859 - val_matthews_correlation: 0.5785\n",
      "\n",
      "Epoch 00019: val_matthews_correlation did not improve from 0.76598\n",
      "Epoch 20/50\n",
      "2322/2322 [==============================] - 1s 526us/step - loss: 0.1139 - matthews_correlation: 0.6154 - val_loss: 0.0803 - val_matthews_correlation: 0.7922\n",
      "\n",
      "Epoch 00020: val_matthews_correlation improved from 0.76598 to 0.79215, saving model to weights_0.h5\n",
      "Epoch 21/50\n",
      "2322/2322 [==============================] - 1s 503us/step - loss: 0.1031 - matthews_correlation: 0.6352 - val_loss: 0.0761 - val_matthews_correlation: 0.8038\n",
      "\n",
      "Epoch 00021: val_matthews_correlation improved from 0.79215 to 0.80383, saving model to weights_0.h5\n",
      "Epoch 22/50\n",
      "2322/2322 [==============================] - 1s 508us/step - loss: 0.1039 - matthews_correlation: 0.6648 - val_loss: 0.0900 - val_matthews_correlation: 0.8187\n",
      "\n",
      "Epoch 00022: val_matthews_correlation improved from 0.80383 to 0.81873, saving model to weights_0.h5\n",
      "Epoch 23/50\n",
      "2322/2322 [==============================] - 1s 508us/step - loss: 0.1000 - matthews_correlation: 0.6176 - val_loss: 0.0772 - val_matthews_correlation: 0.8177\n",
      "\n",
      "Epoch 00023: val_matthews_correlation did not improve from 0.81873\n",
      "Epoch 24/50\n",
      "2322/2322 [==============================] - 1s 504us/step - loss: 0.0999 - matthews_correlation: 0.6569 - val_loss: 0.0772 - val_matthews_correlation: 0.7672\n",
      "\n",
      "Epoch 00024: val_matthews_correlation did not improve from 0.81873\n",
      "Epoch 25/50\n",
      "2322/2322 [==============================] - 1s 504us/step - loss: 0.0993 - matthews_correlation: 0.6534 - val_loss: 0.0928 - val_matthews_correlation: 0.7239\n",
      "\n",
      "Epoch 00025: val_matthews_correlation did not improve from 0.81873\n",
      "Epoch 26/50\n",
      "2322/2322 [==============================] - 1s 501us/step - loss: 0.1091 - matthews_correlation: 0.5847 - val_loss: 0.0793 - val_matthews_correlation: 0.8257\n",
      "\n",
      "Epoch 00026: val_matthews_correlation improved from 0.81873 to 0.82565, saving model to weights_0.h5\n",
      "Epoch 27/50\n",
      "2322/2322 [==============================] - 1s 508us/step - loss: 0.0983 - matthews_correlation: 0.6618 - val_loss: 0.0784 - val_matthews_correlation: 0.7900\n",
      "\n",
      "Epoch 00027: val_matthews_correlation did not improve from 0.82565\n",
      "Epoch 28/50\n",
      "2322/2322 [==============================] - 1s 503us/step - loss: 0.1016 - matthews_correlation: 0.6367 - val_loss: 0.0795 - val_matthews_correlation: 0.8385\n",
      "\n",
      "Epoch 00028: val_matthews_correlation improved from 0.82565 to 0.83847, saving model to weights_0.h5\n",
      "Epoch 29/50\n",
      "2322/2322 [==============================] - 1s 518us/step - loss: 0.1051 - matthews_correlation: 0.6580 - val_loss: 0.0774 - val_matthews_correlation: 0.7537\n",
      "\n",
      "Epoch 00029: val_matthews_correlation did not improve from 0.83847\n",
      "Epoch 30/50\n",
      "2322/2322 [==============================] - 1s 508us/step - loss: 0.0977 - matthews_correlation: 0.6583 - val_loss: 0.0792 - val_matthews_correlation: 0.7823\n",
      "\n",
      "Epoch 00030: val_matthews_correlation did not improve from 0.83847\n",
      "Epoch 31/50\n",
      "2322/2322 [==============================] - 1s 505us/step - loss: 0.0942 - matthews_correlation: 0.6657 - val_loss: 0.0792 - val_matthews_correlation: 0.8385\n",
      "\n",
      "Epoch 00031: val_matthews_correlation did not improve from 0.83847\n",
      "Epoch 32/50\n",
      "2322/2322 [==============================] - 1s 503us/step - loss: 0.0949 - matthews_correlation: 0.6771 - val_loss: 0.0770 - val_matthews_correlation: 0.7711\n",
      "\n",
      "Epoch 00032: val_matthews_correlation did not improve from 0.83847\n",
      "Epoch 33/50\n",
      "2322/2322 [==============================] - 1s 506us/step - loss: 0.0927 - matthews_correlation: 0.6689 - val_loss: 0.0814 - val_matthews_correlation: 0.8150\n",
      "\n",
      "Epoch 00033: val_matthews_correlation did not improve from 0.83847\n",
      "Epoch 34/50\n",
      "2322/2322 [==============================] - 1s 503us/step - loss: 0.0945 - matthews_correlation: 0.6622 - val_loss: 0.0876 - val_matthews_correlation: 0.7527\n",
      "\n",
      "Epoch 00034: val_matthews_correlation did not improve from 0.83847\n",
      "Epoch 35/50\n",
      "2322/2322 [==============================] - 1s 505us/step - loss: 0.0915 - matthews_correlation: 0.6926 - val_loss: 0.1131 - val_matthews_correlation: 0.6607\n",
      "\n",
      "Epoch 00035: val_matthews_correlation did not improve from 0.83847\n",
      "Epoch 36/50\n",
      "2322/2322 [==============================] - 1s 506us/step - loss: 0.1022 - matthews_correlation: 0.6589 - val_loss: 0.0844 - val_matthews_correlation: 0.7241\n",
      "\n",
      "Epoch 00036: val_matthews_correlation did not improve from 0.83847\n",
      "Epoch 37/50\n",
      "2322/2322 [==============================] - 1s 507us/step - loss: 0.0972 - matthews_correlation: 0.6734 - val_loss: 0.0793 - val_matthews_correlation: 0.8051\n",
      "\n",
      "Epoch 00037: val_matthews_correlation did not improve from 0.83847\n",
      "Epoch 38/50\n",
      "2322/2322 [==============================] - 1s 513us/step - loss: 0.0922 - matthews_correlation: 0.6769 - val_loss: 0.0834 - val_matthews_correlation: 0.7621\n",
      "\n",
      "Epoch 00038: val_matthews_correlation did not improve from 0.83847\n",
      "Epoch 39/50\n",
      "2322/2322 [==============================] - 1s 502us/step - loss: 0.0922 - matthews_correlation: 0.6922 - val_loss: 0.0799 - val_matthews_correlation: 0.7983\n",
      "\n",
      "Epoch 00039: val_matthews_correlation did not improve from 0.83847\n",
      "Epoch 40/50\n",
      "2322/2322 [==============================] - 1s 504us/step - loss: 0.0883 - matthews_correlation: 0.7039 - val_loss: 0.0780 - val_matthews_correlation: 0.7442\n",
      "\n",
      "Epoch 00040: val_matthews_correlation did not improve from 0.83847\n",
      "Epoch 41/50\n",
      "2322/2322 [==============================] - 1s 506us/step - loss: 0.0881 - matthews_correlation: 0.7178 - val_loss: 0.0732 - val_matthews_correlation: 0.8300\n",
      "\n",
      "Epoch 00041: val_matthews_correlation did not improve from 0.83847\n",
      "Epoch 42/50\n",
      "2322/2322 [==============================] - 1s 507us/step - loss: 0.0984 - matthews_correlation: 0.6430 - val_loss: 0.0850 - val_matthews_correlation: 0.7296\n",
      "\n",
      "Epoch 00042: val_matthews_correlation did not improve from 0.83847\n",
      "Epoch 43/50\n",
      "2322/2322 [==============================] - 1s 505us/step - loss: 0.0882 - matthews_correlation: 0.7141 - val_loss: 0.0890 - val_matthews_correlation: 0.7546\n",
      "\n",
      "Epoch 00043: val_matthews_correlation did not improve from 0.83847\n",
      "Epoch 44/50\n",
      "2322/2322 [==============================] - 1s 502us/step - loss: 0.0961 - matthews_correlation: 0.6991 - val_loss: 0.0905 - val_matthews_correlation: 0.7189\n",
      "\n",
      "Epoch 00044: val_matthews_correlation did not improve from 0.83847\n",
      "Epoch 45/50\n",
      "2322/2322 [==============================] - 1s 506us/step - loss: 0.0871 - matthews_correlation: 0.6933 - val_loss: 0.0824 - val_matthews_correlation: 0.7813\n",
      "\n",
      "Epoch 00045: val_matthews_correlation did not improve from 0.83847\n",
      "Epoch 46/50\n",
      "2322/2322 [==============================] - 1s 514us/step - loss: 0.0844 - matthews_correlation: 0.7040 - val_loss: 0.0849 - val_matthews_correlation: 0.7781\n",
      "\n",
      "Epoch 00046: val_matthews_correlation did not improve from 0.83847\n",
      "Epoch 47/50\n",
      "2322/2322 [==============================] - 1s 520us/step - loss: 0.0900 - matthews_correlation: 0.6961 - val_loss: 0.0801 - val_matthews_correlation: 0.8144\n",
      "\n",
      "Epoch 00047: val_matthews_correlation did not improve from 0.83847\n",
      "Epoch 48/50\n",
      "2322/2322 [==============================] - 1s 504us/step - loss: 0.0830 - matthews_correlation: 0.7075 - val_loss: 0.0752 - val_matthews_correlation: 0.7879\n",
      "\n",
      "Epoch 00048: val_matthews_correlation did not improve from 0.83847\n",
      "Epoch 49/50\n",
      "2322/2322 [==============================] - 1s 503us/step - loss: 0.0837 - matthews_correlation: 0.7126 - val_loss: 0.0910 - val_matthews_correlation: 0.7471\n",
      "\n",
      "Epoch 00049: val_matthews_correlation did not improve from 0.83847\n",
      "Epoch 50/50\n",
      "2322/2322 [==============================] - 1s 508us/step - loss: 0.0841 - matthews_correlation: 0.6719 - val_loss: 0.0778 - val_matthews_correlation: 0.7780\n",
      "\n",
      "Epoch 00050: val_matthews_correlation did not improve from 0.83847\n",
      "Beginning fold 2\n",
      "Train on 2323 samples, validate on 581 samples\n",
      "Epoch 1/50\n",
      "2323/2323 [==============================] - 2s 1ms/step - loss: 0.3665 - matthews_correlation: 0.0000e+00 - val_loss: 0.2329 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_matthews_correlation improved from -inf to 0.00000, saving model to weights_1.h5\n",
      "Epoch 2/50\n",
      "2323/2323 [==============================] - 1s 505us/step - loss: 0.2283 - matthews_correlation: 0.0000e+00 - val_loss: 0.2289 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00002: val_matthews_correlation did not improve from 0.00000\n",
      "Epoch 3/50\n",
      "2323/2323 [==============================] - 1s 509us/step - loss: 0.2141 - matthews_correlation: 0.0000e+00 - val_loss: 0.2240 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00003: val_matthews_correlation did not improve from 0.00000\n",
      "Epoch 4/50\n",
      "2323/2323 [==============================] - 1s 502us/step - loss: 0.2046 - matthews_correlation: 0.1507 - val_loss: 0.2094 - val_matthews_correlation: 0.1045\n",
      "\n",
      "Epoch 00004: val_matthews_correlation improved from 0.00000 to 0.10453, saving model to weights_1.h5\n",
      "Epoch 5/50\n",
      "2323/2323 [==============================] - 1s 507us/step - loss: 0.1942 - matthews_correlation: 0.3138 - val_loss: 0.2043 - val_matthews_correlation: 0.0640\n",
      "\n",
      "Epoch 00005: val_matthews_correlation did not improve from 0.10453\n",
      "Epoch 6/50\n",
      "2323/2323 [==============================] - 1s 507us/step - loss: 0.1829 - matthews_correlation: 0.2132 - val_loss: 0.1775 - val_matthews_correlation: 0.0747\n",
      "\n",
      "Epoch 00006: val_matthews_correlation did not improve from 0.10453\n",
      "Epoch 7/50\n",
      "2323/2323 [==============================] - 1s 506us/step - loss: 0.1393 - matthews_correlation: 0.2855 - val_loss: 0.1402 - val_matthews_correlation: 0.5825\n",
      "\n",
      "Epoch 00007: val_matthews_correlation improved from 0.10453 to 0.58248, saving model to weights_1.h5\n",
      "Epoch 8/50\n",
      "2323/2323 [==============================] - 1s 509us/step - loss: 0.1364 - matthews_correlation: 0.5363 - val_loss: 0.1334 - val_matthews_correlation: 0.5231\n",
      "\n",
      "Epoch 00008: val_matthews_correlation did not improve from 0.58248\n",
      "Epoch 9/50\n",
      "2323/2323 [==============================] - 1s 502us/step - loss: 0.1186 - matthews_correlation: 0.5693 - val_loss: 0.1370 - val_matthews_correlation: 0.5615\n",
      "\n",
      "Epoch 00009: val_matthews_correlation did not improve from 0.58248\n",
      "Epoch 10/50\n",
      "2323/2323 [==============================] - 1s 506us/step - loss: 0.1208 - matthews_correlation: 0.6311 - val_loss: 0.1144 - val_matthews_correlation: 0.5814\n",
      "\n",
      "Epoch 00010: val_matthews_correlation did not improve from 0.58248\n",
      "Epoch 11/50\n",
      "2323/2323 [==============================] - 1s 507us/step - loss: 0.1050 - matthews_correlation: 0.6155 - val_loss: 0.1129 - val_matthews_correlation: 0.5730\n",
      "\n",
      "Epoch 00011: val_matthews_correlation did not improve from 0.58248\n",
      "Epoch 12/50\n",
      "2323/2323 [==============================] - 1s 529us/step - loss: 0.1087 - matthews_correlation: 0.6300 - val_loss: 0.1152 - val_matthews_correlation: 0.6366\n",
      "\n",
      "Epoch 00012: val_matthews_correlation improved from 0.58248 to 0.63661, saving model to weights_1.h5\n",
      "Epoch 13/50\n",
      "2323/2323 [==============================] - 1s 498us/step - loss: 0.1049 - matthews_correlation: 0.6336 - val_loss: 0.1025 - val_matthews_correlation: 0.5997\n",
      "\n",
      "Epoch 00013: val_matthews_correlation did not improve from 0.63661\n",
      "Epoch 14/50\n",
      "2323/2323 [==============================] - 1s 507us/step - loss: 0.1006 - matthews_correlation: 0.6457 - val_loss: 0.1003 - val_matthews_correlation: 0.6163\n",
      "\n",
      "Epoch 00014: val_matthews_correlation did not improve from 0.63661\n",
      "Epoch 15/50\n",
      "2323/2323 [==============================] - 1s 509us/step - loss: 0.0985 - matthews_correlation: 0.6608 - val_loss: 0.1080 - val_matthews_correlation: 0.6796\n",
      "\n",
      "Epoch 00015: val_matthews_correlation improved from 0.63661 to 0.67964, saving model to weights_1.h5\n",
      "Epoch 16/50\n",
      "2323/2323 [==============================] - 1s 502us/step - loss: 0.1028 - matthews_correlation: 0.6398 - val_loss: 0.1006 - val_matthews_correlation: 0.6384\n",
      "\n",
      "Epoch 00016: val_matthews_correlation did not improve from 0.67964\n",
      "Epoch 17/50\n",
      "2323/2323 [==============================] - 1s 505us/step - loss: 0.0978 - matthews_correlation: 0.6525 - val_loss: 0.1032 - val_matthews_correlation: 0.6345\n",
      "\n",
      "Epoch 00017: val_matthews_correlation did not improve from 0.67964\n",
      "Epoch 18/50\n",
      "2323/2323 [==============================] - 1s 510us/step - loss: 0.1018 - matthews_correlation: 0.6482 - val_loss: 0.1019 - val_matthews_correlation: 0.6410\n",
      "\n",
      "Epoch 00018: val_matthews_correlation did not improve from 0.67964\n",
      "Epoch 19/50\n",
      "2323/2323 [==============================] - 1s 511us/step - loss: 0.1033 - matthews_correlation: 0.6171 - val_loss: 0.1010 - val_matthews_correlation: 0.6679\n",
      "\n",
      "Epoch 00019: val_matthews_correlation did not improve from 0.67964\n",
      "Epoch 20/50\n",
      "2323/2323 [==============================] - 1s 521us/step - loss: 0.1001 - matthews_correlation: 0.6601 - val_loss: 0.1115 - val_matthews_correlation: 0.6680\n",
      "\n",
      "Epoch 00020: val_matthews_correlation did not improve from 0.67964\n",
      "Epoch 21/50\n",
      "2323/2323 [==============================] - 1s 501us/step - loss: 0.0932 - matthews_correlation: 0.6356 - val_loss: 0.1000 - val_matthews_correlation: 0.6314\n",
      "\n",
      "Epoch 00021: val_matthews_correlation did not improve from 0.67964\n",
      "Epoch 22/50\n",
      "2323/2323 [==============================] - 1s 507us/step - loss: 0.0961 - matthews_correlation: 0.6684 - val_loss: 0.1009 - val_matthews_correlation: 0.6185\n",
      "\n",
      "Epoch 00022: val_matthews_correlation did not improve from 0.67964\n",
      "Epoch 23/50\n",
      "2323/2323 [==============================] - 1s 503us/step - loss: 0.0950 - matthews_correlation: 0.6589 - val_loss: 0.1011 - val_matthews_correlation: 0.6010\n",
      "\n",
      "Epoch 00023: val_matthews_correlation did not improve from 0.67964\n",
      "Epoch 24/50\n",
      "2323/2323 [==============================] - 1s 504us/step - loss: 0.0946 - matthews_correlation: 0.6812 - val_loss: 0.1020 - val_matthews_correlation: 0.6794\n",
      "\n",
      "Epoch 00024: val_matthews_correlation did not improve from 0.67964\n",
      "Epoch 25/50\n",
      "2323/2323 [==============================] - 1s 504us/step - loss: 0.0919 - matthews_correlation: 0.6531 - val_loss: 0.1066 - val_matthews_correlation: 0.6290\n",
      "\n",
      "Epoch 00025: val_matthews_correlation did not improve from 0.67964\n",
      "Epoch 26/50\n",
      "2323/2323 [==============================] - 1s 503us/step - loss: 0.0944 - matthews_correlation: 0.7000 - val_loss: 0.1052 - val_matthews_correlation: 0.6118\n",
      "\n",
      "Epoch 00026: val_matthews_correlation did not improve from 0.67964\n",
      "Epoch 27/50\n",
      "2323/2323 [==============================] - 1s 501us/step - loss: 0.0886 - matthews_correlation: 0.7248 - val_loss: 0.1165 - val_matthews_correlation: 0.6031\n",
      "\n",
      "Epoch 00027: val_matthews_correlation did not improve from 0.67964\n",
      "Epoch 28/50\n",
      "2323/2323 [==============================] - 1s 503us/step - loss: 0.0946 - matthews_correlation: 0.6686 - val_loss: 0.1032 - val_matthews_correlation: 0.6410\n",
      "\n",
      "Epoch 00028: val_matthews_correlation did not improve from 0.67964\n",
      "Epoch 29/50\n",
      "2323/2323 [==============================] - 1s 505us/step - loss: 0.0949 - matthews_correlation: 0.6540 - val_loss: 0.0964 - val_matthews_correlation: 0.6930\n",
      "\n",
      "Epoch 00029: val_matthews_correlation improved from 0.67964 to 0.69296, saving model to weights_1.h5\n",
      "Epoch 30/50\n",
      "2323/2323 [==============================] - 1s 505us/step - loss: 0.0955 - matthews_correlation: 0.6747 - val_loss: 0.1046 - val_matthews_correlation: 0.5764\n",
      "\n",
      "Epoch 00030: val_matthews_correlation did not improve from 0.69296\n",
      "Epoch 31/50\n",
      "2323/2323 [==============================] - 1s 512us/step - loss: 0.0906 - matthews_correlation: 0.6849 - val_loss: 0.0973 - val_matthews_correlation: 0.6636\n",
      "\n",
      "Epoch 00031: val_matthews_correlation did not improve from 0.69296\n",
      "Epoch 32/50\n",
      "2323/2323 [==============================] - 1s 506us/step - loss: 0.0936 - matthews_correlation: 0.6650 - val_loss: 0.1189 - val_matthews_correlation: 0.6317\n",
      "\n",
      "Epoch 00032: val_matthews_correlation did not improve from 0.69296\n",
      "Epoch 33/50\n",
      "2323/2323 [==============================] - 1s 500us/step - loss: 0.1148 - matthews_correlation: 0.5517 - val_loss: 0.0941 - val_matthews_correlation: 0.6553\n",
      "\n",
      "Epoch 00033: val_matthews_correlation did not improve from 0.69296\n",
      "Epoch 34/50\n",
      "2323/2323 [==============================] - 1s 501us/step - loss: 0.0934 - matthews_correlation: 0.6690 - val_loss: 0.1130 - val_matthews_correlation: 0.5913\n",
      "\n",
      "Epoch 00034: val_matthews_correlation did not improve from 0.69296\n",
      "Epoch 35/50\n",
      "2323/2323 [==============================] - 1s 505us/step - loss: 0.0902 - matthews_correlation: 0.6867 - val_loss: 0.0963 - val_matthews_correlation: 0.6163\n",
      "\n",
      "Epoch 00035: val_matthews_correlation did not improve from 0.69296\n",
      "Epoch 36/50\n",
      "2323/2323 [==============================] - 1s 503us/step - loss: 0.0898 - matthews_correlation: 0.7030 - val_loss: 0.1010 - val_matthews_correlation: 0.6436\n",
      "\n",
      "Epoch 00036: val_matthews_correlation did not improve from 0.69296\n",
      "Epoch 37/50\n",
      "2323/2323 [==============================] - 1s 504us/step - loss: 0.0888 - matthews_correlation: 0.6933 - val_loss: 0.0976 - val_matthews_correlation: 0.6137\n",
      "\n",
      "Epoch 00037: val_matthews_correlation did not improve from 0.69296\n",
      "Epoch 38/50\n",
      "2323/2323 [==============================] - 1s 506us/step - loss: 0.0912 - matthews_correlation: 0.6281 - val_loss: 0.1035 - val_matthews_correlation: 0.6320\n",
      "\n",
      "Epoch 00038: val_matthews_correlation did not improve from 0.69296\n",
      "Epoch 39/50\n",
      "2323/2323 [==============================] - 1s 504us/step - loss: 0.0865 - matthews_correlation: 0.6844 - val_loss: 0.1126 - val_matthews_correlation: 0.5954\n",
      "\n",
      "Epoch 00039: val_matthews_correlation did not improve from 0.69296\n",
      "Epoch 40/50\n",
      "2323/2323 [==============================] - 1s 521us/step - loss: 0.0846 - matthews_correlation: 0.7147 - val_loss: 0.1012 - val_matthews_correlation: 0.6396\n",
      "\n",
      "Epoch 00040: val_matthews_correlation did not improve from 0.69296\n",
      "Epoch 41/50\n",
      "2323/2323 [==============================] - 1s 496us/step - loss: 0.0823 - matthews_correlation: 0.7066 - val_loss: 0.1015 - val_matthews_correlation: 0.6126\n",
      "\n",
      "Epoch 00041: val_matthews_correlation did not improve from 0.69296\n",
      "Epoch 42/50\n",
      "2323/2323 [==============================] - 1s 506us/step - loss: 0.0934 - matthews_correlation: 0.6725 - val_loss: 0.1173 - val_matthews_correlation: 0.5888\n",
      "\n",
      "Epoch 00042: val_matthews_correlation did not improve from 0.69296\n",
      "Epoch 43/50\n",
      "2323/2323 [==============================] - 1s 501us/step - loss: 0.0826 - matthews_correlation: 0.6951 - val_loss: 0.1002 - val_matthews_correlation: 0.6543\n",
      "\n",
      "Epoch 00043: val_matthews_correlation did not improve from 0.69296\n",
      "Epoch 44/50\n",
      "2323/2323 [==============================] - 1s 501us/step - loss: 0.0805 - matthews_correlation: 0.7349 - val_loss: 0.1129 - val_matthews_correlation: 0.5476\n",
      "\n",
      "Epoch 00044: val_matthews_correlation did not improve from 0.69296\n",
      "Epoch 45/50\n",
      "2323/2323 [==============================] - 1s 517us/step - loss: 0.0829 - matthews_correlation: 0.7250 - val_loss: 0.1184 - val_matthews_correlation: 0.6233\n",
      "\n",
      "Epoch 00045: val_matthews_correlation did not improve from 0.69296\n",
      "Epoch 46/50\n",
      "2323/2323 [==============================] - 1s 504us/step - loss: 0.0809 - matthews_correlation: 0.7237 - val_loss: 0.1081 - val_matthews_correlation: 0.6422\n",
      "\n",
      "Epoch 00046: val_matthews_correlation did not improve from 0.69296\n",
      "Epoch 47/50\n",
      "2323/2323 [==============================] - 1s 503us/step - loss: 0.0815 - matthews_correlation: 0.6896 - val_loss: 0.1220 - val_matthews_correlation: 0.6230\n",
      "\n",
      "Epoch 00047: val_matthews_correlation did not improve from 0.69296\n",
      "Epoch 48/50\n",
      "2323/2323 [==============================] - 1s 526us/step - loss: 0.0832 - matthews_correlation: 0.6845 - val_loss: 0.1148 - val_matthews_correlation: 0.4667\n",
      "\n",
      "Epoch 00048: val_matthews_correlation did not improve from 0.69296\n",
      "Epoch 49/50\n",
      "2323/2323 [==============================] - 1s 494us/step - loss: 0.0887 - matthews_correlation: 0.6653 - val_loss: 0.0991 - val_matthews_correlation: 0.6341\n",
      "\n",
      "Epoch 00049: val_matthews_correlation did not improve from 0.69296\n",
      "Epoch 50/50\n",
      "2323/2323 [==============================] - 1s 504us/step - loss: 0.1037 - matthews_correlation: 0.5807 - val_loss: 0.0988 - val_matthews_correlation: 0.6857\n",
      "\n",
      "Epoch 00050: val_matthews_correlation did not improve from 0.69296\n",
      "Beginning fold 3\n",
      "Train on 2323 samples, validate on 581 samples\n",
      "Epoch 1/50\n",
      "2323/2323 [==============================] - 2s 1ms/step - loss: 0.3659 - matthews_correlation: -0.0016 - val_loss: 0.2391 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_matthews_correlation improved from -inf to 0.00000, saving model to weights_2.h5\n",
      "Epoch 2/50\n",
      "2323/2323 [==============================] - 1s 513us/step - loss: 0.2301 - matthews_correlation: 0.0000e+00 - val_loss: 0.2303 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00002: val_matthews_correlation did not improve from 0.00000\n",
      "Epoch 3/50\n",
      "2323/2323 [==============================] - 1s 513us/step - loss: 0.2300 - matthews_correlation: 0.0000e+00 - val_loss: 0.2248 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00003: val_matthews_correlation did not improve from 0.00000\n",
      "Epoch 4/50\n",
      "2323/2323 [==============================] - 1s 509us/step - loss: 0.2242 - matthews_correlation: 0.0000e+00 - val_loss: 0.2224 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00004: val_matthews_correlation did not improve from 0.00000\n",
      "Epoch 5/50\n",
      "2323/2323 [==============================] - 1s 510us/step - loss: 0.2156 - matthews_correlation: 0.0000e+00 - val_loss: 0.2021 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00005: val_matthews_correlation did not improve from 0.00000\n",
      "Epoch 6/50\n",
      "2323/2323 [==============================] - 1s 501us/step - loss: 0.2090 - matthews_correlation: 0.0000e+00 - val_loss: 0.1925 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00006: val_matthews_correlation did not improve from 0.00000\n",
      "Epoch 7/50\n",
      "2323/2323 [==============================] - 1s 505us/step - loss: 0.1907 - matthews_correlation: -0.0013 - val_loss: 0.1643 - val_matthews_correlation: 0.3119\n",
      "\n",
      "Epoch 00007: val_matthews_correlation improved from 0.00000 to 0.31186, saving model to weights_2.h5\n",
      "Epoch 8/50\n",
      "2323/2323 [==============================] - 1s 511us/step - loss: 0.1732 - matthews_correlation: 0.0193 - val_loss: 0.1483 - val_matthews_correlation: 0.4035\n",
      "\n",
      "Epoch 00008: val_matthews_correlation improved from 0.31186 to 0.40349, saving model to weights_2.h5\n",
      "Epoch 9/50\n",
      "2323/2323 [==============================] - 1s 507us/step - loss: 0.1364 - matthews_correlation: 0.4454 - val_loss: 0.1295 - val_matthews_correlation: 0.1950\n",
      "\n",
      "Epoch 00009: val_matthews_correlation did not improve from 0.40349\n",
      "Epoch 10/50\n",
      "2323/2323 [==============================] - 1s 509us/step - loss: 0.1113 - matthews_correlation: 0.6181 - val_loss: 0.1180 - val_matthews_correlation: 0.5876\n",
      "\n",
      "Epoch 00010: val_matthews_correlation improved from 0.40349 to 0.58757, saving model to weights_2.h5\n",
      "Epoch 11/50\n",
      "2323/2323 [==============================] - 1s 509us/step - loss: 0.1060 - matthews_correlation: 0.6358 - val_loss: 0.1272 - val_matthews_correlation: 0.5028\n",
      "\n",
      "Epoch 00011: val_matthews_correlation did not improve from 0.58757\n",
      "Epoch 12/50\n",
      "2323/2323 [==============================] - 1s 510us/step - loss: 0.1020 - matthews_correlation: 0.6227 - val_loss: 0.1110 - val_matthews_correlation: 0.6813\n",
      "\n",
      "Epoch 00012: val_matthews_correlation improved from 0.58757 to 0.68128, saving model to weights_2.h5\n",
      "Epoch 13/50\n",
      "2323/2323 [==============================] - 1s 507us/step - loss: 0.0981 - matthews_correlation: 0.6486 - val_loss: 0.1136 - val_matthews_correlation: 0.4713\n",
      "\n",
      "Epoch 00013: val_matthews_correlation did not improve from 0.68128\n",
      "Epoch 14/50\n",
      "2323/2323 [==============================] - 1s 522us/step - loss: 0.0990 - matthews_correlation: 0.6726 - val_loss: 0.1108 - val_matthews_correlation: 0.4851\n",
      "\n",
      "Epoch 00014: val_matthews_correlation did not improve from 0.68128\n",
      "Epoch 15/50\n",
      "2323/2323 [==============================] - 1s 500us/step - loss: 0.0959 - matthews_correlation: 0.6778 - val_loss: 0.1146 - val_matthews_correlation: 0.5872\n",
      "\n",
      "Epoch 00015: val_matthews_correlation did not improve from 0.68128\n",
      "Epoch 16/50\n",
      "2323/2323 [==============================] - 1s 509us/step - loss: 0.0991 - matthews_correlation: 0.7225 - val_loss: 0.1223 - val_matthews_correlation: 0.3717\n",
      "\n",
      "Epoch 00016: val_matthews_correlation did not improve from 0.68128\n",
      "Epoch 17/50\n",
      "2323/2323 [==============================] - 1s 511us/step - loss: 0.0988 - matthews_correlation: 0.6419 - val_loss: 0.1138 - val_matthews_correlation: 0.5227\n",
      "\n",
      "Epoch 00017: val_matthews_correlation did not improve from 0.68128\n",
      "Epoch 18/50\n",
      "2323/2323 [==============================] - 1s 509us/step - loss: 0.0932 - matthews_correlation: 0.6519 - val_loss: 0.1151 - val_matthews_correlation: 0.5590\n",
      "\n",
      "Epoch 00018: val_matthews_correlation did not improve from 0.68128\n",
      "Epoch 19/50\n",
      "2323/2323 [==============================] - 1s 511us/step - loss: 0.0904 - matthews_correlation: 0.6743 - val_loss: 0.1195 - val_matthews_correlation: 0.5223\n",
      "\n",
      "Epoch 00019: val_matthews_correlation did not improve from 0.68128\n",
      "Epoch 20/50\n",
      "2323/2323 [==============================] - 1s 508us/step - loss: 0.0929 - matthews_correlation: 0.6803 - val_loss: 0.1110 - val_matthews_correlation: 0.5945\n",
      "\n",
      "Epoch 00020: val_matthews_correlation did not improve from 0.68128\n",
      "Epoch 21/50\n",
      "2323/2323 [==============================] - 1s 510us/step - loss: 0.0878 - matthews_correlation: 0.7154 - val_loss: 0.1137 - val_matthews_correlation: 0.5588\n",
      "\n",
      "Epoch 00021: val_matthews_correlation did not improve from 0.68128\n",
      "Epoch 22/50\n",
      "2323/2323 [==============================] - 1s 527us/step - loss: 0.0890 - matthews_correlation: 0.6953 - val_loss: 0.1153 - val_matthews_correlation: 0.5414\n",
      "\n",
      "Epoch 00022: val_matthews_correlation did not improve from 0.68128\n",
      "Epoch 23/50\n",
      "2323/2323 [==============================] - 1s 509us/step - loss: 0.0840 - matthews_correlation: 0.7112 - val_loss: 0.1192 - val_matthews_correlation: 0.4993\n",
      "\n",
      "Epoch 00023: val_matthews_correlation did not improve from 0.68128\n",
      "Epoch 24/50\n",
      "2323/2323 [==============================] - 1s 537us/step - loss: 0.0886 - matthews_correlation: 0.6660 - val_loss: 0.1192 - val_matthews_correlation: 0.5020\n",
      "\n",
      "Epoch 00024: val_matthews_correlation did not improve from 0.68128\n",
      "Epoch 25/50\n",
      "2323/2323 [==============================] - 1s 507us/step - loss: 0.0934 - matthews_correlation: 0.6866 - val_loss: 0.1135 - val_matthews_correlation: 0.5906\n",
      "\n",
      "Epoch 00025: val_matthews_correlation did not improve from 0.68128\n",
      "Epoch 26/50\n",
      "2323/2323 [==============================] - 1s 505us/step - loss: 0.0840 - matthews_correlation: 0.7256 - val_loss: 0.1165 - val_matthews_correlation: 0.5400\n",
      "\n",
      "Epoch 00026: val_matthews_correlation did not improve from 0.68128\n",
      "Epoch 27/50\n",
      "2323/2323 [==============================] - 1s 506us/step - loss: 0.0799 - matthews_correlation: 0.7437 - val_loss: 0.1167 - val_matthews_correlation: 0.5906\n",
      "\n",
      "Epoch 00027: val_matthews_correlation did not improve from 0.68128\n",
      "Epoch 28/50\n",
      "2323/2323 [==============================] - 1s 508us/step - loss: 0.0840 - matthews_correlation: 0.7048 - val_loss: 0.1163 - val_matthews_correlation: 0.5754\n",
      "\n",
      "Epoch 00028: val_matthews_correlation did not improve from 0.68128\n",
      "Epoch 29/50\n",
      "2323/2323 [==============================] - 1s 505us/step - loss: 0.0784 - matthews_correlation: 0.7733 - val_loss: 0.1429 - val_matthews_correlation: 0.6058\n",
      "\n",
      "Epoch 00029: val_matthews_correlation did not improve from 0.68128\n",
      "Epoch 30/50\n",
      "2323/2323 [==============================] - 1s 505us/step - loss: 0.0848 - matthews_correlation: 0.7181 - val_loss: 0.1198 - val_matthews_correlation: 0.6132\n",
      "\n",
      "Epoch 00030: val_matthews_correlation did not improve from 0.68128\n",
      "Epoch 31/50\n",
      "2323/2323 [==============================] - 1s 507us/step - loss: 0.0780 - matthews_correlation: 0.7482 - val_loss: 0.1330 - val_matthews_correlation: 0.5774\n",
      "\n",
      "Epoch 00031: val_matthews_correlation did not improve from 0.68128\n",
      "Epoch 32/50\n",
      "2323/2323 [==============================] - 1s 511us/step - loss: 0.0750 - matthews_correlation: 0.7567 - val_loss: 0.1234 - val_matthews_correlation: 0.5364\n",
      "\n",
      "Epoch 00032: val_matthews_correlation did not improve from 0.68128\n",
      "Epoch 33/50\n",
      "2323/2323 [==============================] - 1s 514us/step - loss: 0.0999 - matthews_correlation: 0.6788 - val_loss: 0.1145 - val_matthews_correlation: 0.6299\n",
      "\n",
      "Epoch 00033: val_matthews_correlation did not improve from 0.68128\n",
      "Epoch 34/50\n",
      "2323/2323 [==============================] - 1s 508us/step - loss: 0.0818 - matthews_correlation: 0.7086 - val_loss: 0.1247 - val_matthews_correlation: 0.3495\n",
      "\n",
      "Epoch 00034: val_matthews_correlation did not improve from 0.68128\n",
      "Epoch 35/50\n",
      "2323/2323 [==============================] - 1s 509us/step - loss: 0.0827 - matthews_correlation: 0.7456 - val_loss: 0.1253 - val_matthews_correlation: 0.5794\n",
      "\n",
      "Epoch 00035: val_matthews_correlation did not improve from 0.68128\n",
      "Epoch 36/50\n",
      "2323/2323 [==============================] - 1s 506us/step - loss: 0.1001 - matthews_correlation: 0.6904 - val_loss: 0.1174 - val_matthews_correlation: 0.5726\n",
      "\n",
      "Epoch 00036: val_matthews_correlation did not improve from 0.68128\n",
      "Epoch 37/50\n",
      "2323/2323 [==============================] - 1s 509us/step - loss: 0.0888 - matthews_correlation: 0.7316 - val_loss: 0.1129 - val_matthews_correlation: 0.5872\n",
      "\n",
      "Epoch 00037: val_matthews_correlation did not improve from 0.68128\n",
      "Epoch 38/50\n",
      "2323/2323 [==============================] - 1s 505us/step - loss: 0.0995 - matthews_correlation: 0.6571 - val_loss: 0.1083 - val_matthews_correlation: 0.5923\n",
      "\n",
      "Epoch 00038: val_matthews_correlation did not improve from 0.68128\n",
      "Epoch 39/50\n",
      "2323/2323 [==============================] - 1s 510us/step - loss: 0.0821 - matthews_correlation: 0.7042 - val_loss: 0.1139 - val_matthews_correlation: 0.6161\n",
      "\n",
      "Epoch 00039: val_matthews_correlation did not improve from 0.68128\n",
      "Epoch 40/50\n",
      "2323/2323 [==============================] - 1s 506us/step - loss: 0.0780 - matthews_correlation: 0.7302 - val_loss: 0.1141 - val_matthews_correlation: 0.6944\n",
      "\n",
      "Epoch 00040: val_matthews_correlation improved from 0.68128 to 0.69435, saving model to weights_2.h5\n",
      "Epoch 41/50\n",
      "2323/2323 [==============================] - 1s 511us/step - loss: 0.0952 - matthews_correlation: 0.6701 - val_loss: 0.1092 - val_matthews_correlation: 0.6167\n",
      "\n",
      "Epoch 00041: val_matthews_correlation did not improve from 0.69435\n",
      "Epoch 42/50\n",
      "2323/2323 [==============================] - 1s 520us/step - loss: 0.0864 - matthews_correlation: 0.6918 - val_loss: 0.1188 - val_matthews_correlation: 0.6534\n",
      "\n",
      "Epoch 00042: val_matthews_correlation did not improve from 0.69435\n",
      "Epoch 43/50\n",
      "2323/2323 [==============================] - 1s 507us/step - loss: 0.0799 - matthews_correlation: 0.6971 - val_loss: 0.1113 - val_matthews_correlation: 0.6657\n",
      "\n",
      "Epoch 00043: val_matthews_correlation did not improve from 0.69435\n",
      "Epoch 44/50\n",
      "2323/2323 [==============================] - 1s 506us/step - loss: 0.0803 - matthews_correlation: 0.7617 - val_loss: 0.1087 - val_matthews_correlation: 0.6312\n",
      "\n",
      "Epoch 00044: val_matthews_correlation did not improve from 0.69435\n",
      "Epoch 45/50\n",
      "2323/2323 [==============================] - 1s 509us/step - loss: 0.0762 - matthews_correlation: 0.7006 - val_loss: 0.1113 - val_matthews_correlation: 0.6147\n",
      "\n",
      "Epoch 00045: val_matthews_correlation did not improve from 0.69435\n",
      "Epoch 46/50\n",
      "2323/2323 [==============================] - 1s 510us/step - loss: 0.0772 - matthews_correlation: 0.7231 - val_loss: 0.1084 - val_matthews_correlation: 0.6185\n",
      "\n",
      "Epoch 00046: val_matthews_correlation did not improve from 0.69435\n",
      "Epoch 47/50\n",
      "2323/2323 [==============================] - 1s 503us/step - loss: 0.0814 - matthews_correlation: 0.7349 - val_loss: 0.1168 - val_matthews_correlation: 0.6054\n",
      "\n",
      "Epoch 00047: val_matthews_correlation did not improve from 0.69435\n",
      "Epoch 48/50\n",
      "2323/2323 [==============================] - 1s 505us/step - loss: 0.0787 - matthews_correlation: 0.7569 - val_loss: 0.1143 - val_matthews_correlation: 0.6167\n",
      "\n",
      "Epoch 00048: val_matthews_correlation did not improve from 0.69435\n",
      "Epoch 49/50\n",
      "2323/2323 [==============================] - 1s 519us/step - loss: 0.0843 - matthews_correlation: 0.7224 - val_loss: 0.1283 - val_matthews_correlation: 0.5588\n",
      "\n",
      "Epoch 00049: val_matthews_correlation did not improve from 0.69435\n",
      "Epoch 50/50\n",
      "2323/2323 [==============================] - 1s 505us/step - loss: 0.0780 - matthews_correlation: 0.7224 - val_loss: 0.1074 - val_matthews_correlation: 0.6608\n",
      "\n",
      "Epoch 00050: val_matthews_correlation did not improve from 0.69435\n",
      "Beginning fold 4\n",
      "Train on 2324 samples, validate on 580 samples\n",
      "Epoch 1/50\n",
      "2324/2324 [==============================] - 2s 1ms/step - loss: 0.3407 - matthews_correlation: 0.0100 - val_loss: 0.2404 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_matthews_correlation improved from -inf to 0.00000, saving model to weights_3.h5\n",
      "Epoch 2/50\n",
      "2324/2324 [==============================] - 1s 507us/step - loss: 0.2306 - matthews_correlation: 0.0000e+00 - val_loss: 0.2175 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00002: val_matthews_correlation did not improve from 0.00000\n",
      "Epoch 3/50\n",
      "2324/2324 [==============================] - 1s 500us/step - loss: 0.2201 - matthews_correlation: 0.0000e+00 - val_loss: 0.2095 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00003: val_matthews_correlation did not improve from 0.00000\n",
      "Epoch 4/50\n",
      "2324/2324 [==============================] - 1s 507us/step - loss: 0.2145 - matthews_correlation: 0.0000e+00 - val_loss: 0.2177 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00004: val_matthews_correlation did not improve from 0.00000\n",
      "Epoch 5/50\n",
      "2324/2324 [==============================] - 1s 504us/step - loss: 0.2064 - matthews_correlation: 0.0000e+00 - val_loss: 0.1902 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00005: val_matthews_correlation did not improve from 0.00000\n",
      "Epoch 6/50\n",
      "2324/2324 [==============================] - 1s 507us/step - loss: 0.1834 - matthews_correlation: 0.1818 - val_loss: 0.1842 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00006: val_matthews_correlation did not improve from 0.00000\n",
      "Epoch 7/50\n",
      "2324/2324 [==============================] - 1s 508us/step - loss: 0.1654 - matthews_correlation: 0.1635 - val_loss: 0.1397 - val_matthews_correlation: 0.3059\n",
      "\n",
      "Epoch 00007: val_matthews_correlation improved from 0.00000 to 0.30591, saving model to weights_3.h5\n",
      "Epoch 8/50\n",
      "2324/2324 [==============================] - 1s 503us/step - loss: 0.1412 - matthews_correlation: 0.3178 - val_loss: 0.1356 - val_matthews_correlation: 0.3133\n",
      "\n",
      "Epoch 00008: val_matthews_correlation improved from 0.30591 to 0.31327, saving model to weights_3.h5\n",
      "Epoch 9/50\n",
      "2324/2324 [==============================] - 1s 503us/step - loss: 0.1218 - matthews_correlation: 0.5635 - val_loss: 0.1174 - val_matthews_correlation: 0.4773\n",
      "\n",
      "Epoch 00009: val_matthews_correlation improved from 0.31327 to 0.47726, saving model to weights_3.h5\n",
      "Epoch 10/50\n",
      "2324/2324 [==============================] - 1s 507us/step - loss: 0.1165 - matthews_correlation: 0.6374 - val_loss: 0.1137 - val_matthews_correlation: 0.4936\n",
      "\n",
      "Epoch 00010: val_matthews_correlation improved from 0.47726 to 0.49355, saving model to weights_3.h5\n",
      "Epoch 11/50\n",
      "2324/2324 [==============================] - 1s 506us/step - loss: 0.1057 - matthews_correlation: 0.5833 - val_loss: 0.1054 - val_matthews_correlation: 0.6353\n",
      "\n",
      "Epoch 00011: val_matthews_correlation improved from 0.49355 to 0.63531, saving model to weights_3.h5\n",
      "Epoch 12/50\n",
      "2324/2324 [==============================] - 1s 501us/step - loss: 0.1102 - matthews_correlation: 0.6159 - val_loss: 0.1258 - val_matthews_correlation: 0.4244\n",
      "\n",
      "Epoch 00012: val_matthews_correlation did not improve from 0.63531\n",
      "Epoch 13/50\n",
      "2324/2324 [==============================] - 1s 509us/step - loss: 0.1042 - matthews_correlation: 0.6300 - val_loss: 0.1085 - val_matthews_correlation: 0.6875\n",
      "\n",
      "Epoch 00013: val_matthews_correlation improved from 0.63531 to 0.68751, saving model to weights_3.h5\n",
      "Epoch 14/50\n",
      "2324/2324 [==============================] - 1s 503us/step - loss: 0.1033 - matthews_correlation: 0.6730 - val_loss: 0.1080 - val_matthews_correlation: 0.4599\n",
      "\n",
      "Epoch 00014: val_matthews_correlation did not improve from 0.68751\n",
      "Epoch 15/50\n",
      "2324/2324 [==============================] - 1s 503us/step - loss: 0.1002 - matthews_correlation: 0.6617 - val_loss: 0.1049 - val_matthews_correlation: 0.6127\n",
      "\n",
      "Epoch 00015: val_matthews_correlation did not improve from 0.68751\n",
      "Epoch 16/50\n",
      "2324/2324 [==============================] - 1s 527us/step - loss: 0.1055 - matthews_correlation: 0.6590 - val_loss: 0.1007 - val_matthews_correlation: 0.5700\n",
      "\n",
      "Epoch 00016: val_matthews_correlation did not improve from 0.68751\n",
      "Epoch 17/50\n",
      "2324/2324 [==============================] - 1s 504us/step - loss: 0.0972 - matthews_correlation: 0.6618 - val_loss: 0.1231 - val_matthews_correlation: 0.7066\n",
      "\n",
      "Epoch 00017: val_matthews_correlation improved from 0.68751 to 0.70664, saving model to weights_3.h5\n",
      "Epoch 18/50\n",
      "2324/2324 [==============================] - 1s 503us/step - loss: 0.0999 - matthews_correlation: 0.7110 - val_loss: 0.1216 - val_matthews_correlation: 0.4696\n",
      "\n",
      "Epoch 00018: val_matthews_correlation did not improve from 0.70664\n",
      "Epoch 19/50\n",
      "2324/2324 [==============================] - 1s 505us/step - loss: 0.0953 - matthews_correlation: 0.6674 - val_loss: 0.1009 - val_matthews_correlation: 0.7185\n",
      "\n",
      "Epoch 00019: val_matthews_correlation improved from 0.70664 to 0.71846, saving model to weights_3.h5\n",
      "Epoch 20/50\n",
      "2324/2324 [==============================] - 1s 500us/step - loss: 0.0920 - matthews_correlation: 0.7361 - val_loss: 0.1234 - val_matthews_correlation: 0.4927\n",
      "\n",
      "Epoch 00020: val_matthews_correlation did not improve from 0.71846\n",
      "Epoch 21/50\n",
      "2324/2324 [==============================] - 1s 508us/step - loss: 0.1028 - matthews_correlation: 0.6646 - val_loss: 0.1412 - val_matthews_correlation: 0.4244\n",
      "\n",
      "Epoch 00021: val_matthews_correlation did not improve from 0.71846\n",
      "Epoch 22/50\n",
      "2324/2324 [==============================] - 1s 516us/step - loss: 0.1021 - matthews_correlation: 0.6233 - val_loss: 0.1064 - val_matthews_correlation: 0.5357\n",
      "\n",
      "Epoch 00022: val_matthews_correlation did not improve from 0.71846\n",
      "Epoch 23/50\n",
      "2324/2324 [==============================] - 1s 505us/step - loss: 0.0925 - matthews_correlation: 0.7014 - val_loss: 0.1176 - val_matthews_correlation: 0.5146\n",
      "\n",
      "Epoch 00023: val_matthews_correlation did not improve from 0.71846\n",
      "Epoch 24/50\n",
      "2324/2324 [==============================] - 1s 505us/step - loss: 0.0918 - matthews_correlation: 0.6956 - val_loss: 0.1074 - val_matthews_correlation: 0.6426\n",
      "\n",
      "Epoch 00024: val_matthews_correlation did not improve from 0.71846\n",
      "Epoch 25/50\n",
      "2324/2324 [==============================] - 1s 519us/step - loss: 0.0925 - matthews_correlation: 0.7036 - val_loss: 0.1027 - val_matthews_correlation: 0.6477\n",
      "\n",
      "Epoch 00025: val_matthews_correlation did not improve from 0.71846\n",
      "Epoch 26/50\n",
      "2324/2324 [==============================] - 1s 497us/step - loss: 0.0900 - matthews_correlation: 0.7004 - val_loss: 0.1017 - val_matthews_correlation: 0.6822\n",
      "\n",
      "Epoch 00026: val_matthews_correlation did not improve from 0.71846\n",
      "Epoch 27/50\n",
      "2324/2324 [==============================] - 1s 504us/step - loss: 0.0939 - matthews_correlation: 0.7080 - val_loss: 0.1298 - val_matthews_correlation: 0.4388\n",
      "\n",
      "Epoch 00027: val_matthews_correlation did not improve from 0.71846\n",
      "Epoch 28/50\n",
      "2324/2324 [==============================] - 1s 505us/step - loss: 0.0981 - matthews_correlation: 0.6749 - val_loss: 0.1146 - val_matthews_correlation: 0.5288\n",
      "\n",
      "Epoch 00028: val_matthews_correlation did not improve from 0.71846\n",
      "Epoch 29/50\n",
      "2324/2324 [==============================] - 1s 505us/step - loss: 0.0923 - matthews_correlation: 0.7107 - val_loss: 0.1090 - val_matthews_correlation: 0.7349\n",
      "\n",
      "Epoch 00029: val_matthews_correlation improved from 0.71846 to 0.73486, saving model to weights_3.h5\n",
      "Epoch 30/50\n",
      "2324/2324 [==============================] - 1s 505us/step - loss: 0.1105 - matthews_correlation: 0.5887 - val_loss: 0.1128 - val_matthews_correlation: 0.4635\n",
      "\n",
      "Epoch 00030: val_matthews_correlation did not improve from 0.73486\n",
      "Epoch 31/50\n",
      "2324/2324 [==============================] - 1s 506us/step - loss: 0.0981 - matthews_correlation: 0.6973 - val_loss: 0.0924 - val_matthews_correlation: 0.6811\n",
      "\n",
      "Epoch 00031: val_matthews_correlation did not improve from 0.73486\n",
      "Epoch 32/50\n",
      "2324/2324 [==============================] - 1s 505us/step - loss: 0.0908 - matthews_correlation: 0.7178 - val_loss: 0.1003 - val_matthews_correlation: 0.7707\n",
      "\n",
      "Epoch 00032: val_matthews_correlation improved from 0.73486 to 0.77068, saving model to weights_3.h5\n",
      "Epoch 33/50\n",
      "2324/2324 [==============================] - 1s 507us/step - loss: 0.0941 - matthews_correlation: 0.6926 - val_loss: 0.1010 - val_matthews_correlation: 0.6346\n",
      "\n",
      "Epoch 00033: val_matthews_correlation did not improve from 0.77068\n",
      "Epoch 34/50\n",
      "2324/2324 [==============================] - 1s 504us/step - loss: 0.0876 - matthews_correlation: 0.7033 - val_loss: 0.1098 - val_matthews_correlation: 0.5163\n",
      "\n",
      "Epoch 00034: val_matthews_correlation did not improve from 0.77068\n",
      "Epoch 35/50\n",
      "2324/2324 [==============================] - 1s 510us/step - loss: 0.0929 - matthews_correlation: 0.6823 - val_loss: 0.1115 - val_matthews_correlation: 0.4791\n",
      "\n",
      "Epoch 00035: val_matthews_correlation did not improve from 0.77068\n",
      "Epoch 36/50\n",
      "2324/2324 [==============================] - 1s 508us/step - loss: 0.0947 - matthews_correlation: 0.6513 - val_loss: 0.0961 - val_matthews_correlation: 0.6981\n",
      "\n",
      "Epoch 00036: val_matthews_correlation did not improve from 0.77068\n",
      "Epoch 37/50\n",
      "2324/2324 [==============================] - 1s 502us/step - loss: 0.0901 - matthews_correlation: 0.7251 - val_loss: 0.1051 - val_matthews_correlation: 0.4645\n",
      "\n",
      "Epoch 00037: val_matthews_correlation did not improve from 0.77068\n",
      "Epoch 38/50\n",
      "2324/2324 [==============================] - 1s 508us/step - loss: 0.0875 - matthews_correlation: 0.6808 - val_loss: 0.1013 - val_matthews_correlation: 0.6035\n",
      "\n",
      "Epoch 00038: val_matthews_correlation did not improve from 0.77068\n",
      "Epoch 39/50\n",
      "2324/2324 [==============================] - 1s 503us/step - loss: 0.0848 - matthews_correlation: 0.7107 - val_loss: 0.1088 - val_matthews_correlation: 0.6945\n",
      "\n",
      "Epoch 00039: val_matthews_correlation did not improve from 0.77068\n",
      "Epoch 40/50\n",
      "2324/2324 [==============================] - 1s 506us/step - loss: 0.0930 - matthews_correlation: 0.7249 - val_loss: 0.1008 - val_matthews_correlation: 0.6035\n",
      "\n",
      "Epoch 00040: val_matthews_correlation did not improve from 0.77068\n",
      "Epoch 41/50\n",
      "2324/2324 [==============================] - 1s 502us/step - loss: 0.0863 - matthews_correlation: 0.7464 - val_loss: 0.1022 - val_matthews_correlation: 0.5806\n",
      "\n",
      "Epoch 00041: val_matthews_correlation did not improve from 0.77068\n",
      "Epoch 42/50\n",
      "2324/2324 [==============================] - 1s 505us/step - loss: 0.0827 - matthews_correlation: 0.7166 - val_loss: 0.1024 - val_matthews_correlation: 0.6470\n",
      "\n",
      "Epoch 00042: val_matthews_correlation did not improve from 0.77068\n",
      "Epoch 43/50\n",
      "2324/2324 [==============================] - 1s 506us/step - loss: 0.0862 - matthews_correlation: 0.7163 - val_loss: 0.1071 - val_matthews_correlation: 0.5143\n",
      "\n",
      "Epoch 00043: val_matthews_correlation did not improve from 0.77068\n",
      "Epoch 44/50\n",
      "2324/2324 [==============================] - 1s 502us/step - loss: 0.0901 - matthews_correlation: 0.7148 - val_loss: 0.1119 - val_matthews_correlation: 0.5582\n",
      "\n",
      "Epoch 00044: val_matthews_correlation did not improve from 0.77068\n",
      "Epoch 45/50\n",
      "2324/2324 [==============================] - 1s 506us/step - loss: 0.0870 - matthews_correlation: 0.6848 - val_loss: 0.1044 - val_matthews_correlation: 0.5528\n",
      "\n",
      "Epoch 00045: val_matthews_correlation did not improve from 0.77068\n",
      "Epoch 46/50\n",
      "2324/2324 [==============================] - 1s 505us/step - loss: 0.0877 - matthews_correlation: 0.6925 - val_loss: 0.1054 - val_matthews_correlation: 0.6981\n",
      "\n",
      "Epoch 00046: val_matthews_correlation did not improve from 0.77068\n",
      "Epoch 47/50\n",
      "2324/2324 [==============================] - 1s 504us/step - loss: 0.0866 - matthews_correlation: 0.6565 - val_loss: 0.1060 - val_matthews_correlation: 0.5844\n",
      "\n",
      "Epoch 00047: val_matthews_correlation did not improve from 0.77068\n",
      "Epoch 48/50\n",
      "2324/2324 [==============================] - 1s 500us/step - loss: 0.0870 - matthews_correlation: 0.7188 - val_loss: 0.1041 - val_matthews_correlation: 0.6290\n",
      "\n",
      "Epoch 00048: val_matthews_correlation did not improve from 0.77068\n",
      "Epoch 49/50\n",
      "2324/2324 [==============================] - 1s 522us/step - loss: 0.0815 - matthews_correlation: 0.7417 - val_loss: 0.0966 - val_matthews_correlation: 0.6225\n",
      "\n",
      "Epoch 00049: val_matthews_correlation did not improve from 0.77068\n",
      "Epoch 50/50\n",
      "2324/2324 [==============================] - 1s 494us/step - loss: 0.0818 - matthews_correlation: 0.7093 - val_loss: 0.0987 - val_matthews_correlation: 0.7239\n",
      "\n",
      "Epoch 00050: val_matthews_correlation did not improve from 0.77068\n",
      "Beginning fold 5\n",
      "Train on 2324 samples, validate on 580 samples\n",
      "Epoch 1/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.3936 - matthews_correlation: 0.0000e+00 - val_loss: 0.2296 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_matthews_correlation improved from -inf to 0.00000, saving model to weights_4.h5\n",
      "Epoch 2/50\n",
      "2324/2324 [==============================] - 1s 505us/step - loss: 0.2318 - matthews_correlation: 0.0000e+00 - val_loss: 0.2248 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00002: val_matthews_correlation did not improve from 0.00000\n",
      "Epoch 3/50\n",
      "2324/2324 [==============================] - 1s 503us/step - loss: 0.2273 - matthews_correlation: 0.0000e+00 - val_loss: 0.2304 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00003: val_matthews_correlation did not improve from 0.00000\n",
      "Epoch 4/50\n",
      "2324/2324 [==============================] - 1s 503us/step - loss: 0.2121 - matthews_correlation: 0.0000e+00 - val_loss: 0.2222 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00004: val_matthews_correlation did not improve from 0.00000\n",
      "Epoch 5/50\n",
      "2324/2324 [==============================] - 1s 508us/step - loss: 0.2312 - matthews_correlation: 0.0000e+00 - val_loss: 0.1993 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00005: val_matthews_correlation did not improve from 0.00000\n",
      "Epoch 6/50\n",
      "2324/2324 [==============================] - 1s 502us/step - loss: 0.1966 - matthews_correlation: 0.0000e+00 - val_loss: 0.1770 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00006: val_matthews_correlation did not improve from 0.00000\n",
      "Epoch 7/50\n",
      "2324/2324 [==============================] - 1s 506us/step - loss: 0.1664 - matthews_correlation: 0.1247 - val_loss: 0.1705 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00007: val_matthews_correlation did not improve from 0.00000\n",
      "Epoch 8/50\n",
      "2324/2324 [==============================] - 1s 509us/step - loss: 0.1764 - matthews_correlation: 0.1469 - val_loss: 0.1510 - val_matthews_correlation: 0.4580\n",
      "\n",
      "Epoch 00008: val_matthews_correlation improved from 0.00000 to 0.45805, saving model to weights_4.h5\n",
      "Epoch 9/50\n",
      "2324/2324 [==============================] - 1s 511us/step - loss: 0.1322 - matthews_correlation: 0.5739 - val_loss: 0.1474 - val_matthews_correlation: 0.5312\n",
      "\n",
      "Epoch 00009: val_matthews_correlation improved from 0.45805 to 0.53118, saving model to weights_4.h5\n",
      "Epoch 10/50\n",
      "2324/2324 [==============================] - 1s 510us/step - loss: 0.1200 - matthews_correlation: 0.6170 - val_loss: 0.1357 - val_matthews_correlation: 0.5890\n",
      "\n",
      "Epoch 00010: val_matthews_correlation improved from 0.53118 to 0.58900, saving model to weights_4.h5\n",
      "Epoch 11/50\n",
      "2324/2324 [==============================] - 1s 500us/step - loss: 0.1158 - matthews_correlation: 0.5276 - val_loss: 0.1342 - val_matthews_correlation: 0.3478\n",
      "\n",
      "Epoch 00011: val_matthews_correlation did not improve from 0.58900\n",
      "Epoch 12/50\n",
      "2324/2324 [==============================] - 1s 507us/step - loss: 0.1075 - matthews_correlation: 0.6156 - val_loss: 0.1307 - val_matthews_correlation: 0.5609\n",
      "\n",
      "Epoch 00012: val_matthews_correlation did not improve from 0.58900\n",
      "Epoch 13/50\n",
      "2324/2324 [==============================] - 1s 504us/step - loss: 0.1037 - matthews_correlation: 0.6711 - val_loss: 0.1389 - val_matthews_correlation: 0.4692\n",
      "\n",
      "Epoch 00013: val_matthews_correlation did not improve from 0.58900\n",
      "Epoch 14/50\n",
      "2324/2324 [==============================] - 1s 507us/step - loss: 0.1064 - matthews_correlation: 0.6066 - val_loss: 0.1188 - val_matthews_correlation: 0.6049\n",
      "\n",
      "Epoch 00014: val_matthews_correlation improved from 0.58900 to 0.60493, saving model to weights_4.h5\n",
      "Epoch 15/50\n",
      "2324/2324 [==============================] - 1s 505us/step - loss: 0.1083 - matthews_correlation: 0.6414 - val_loss: 0.1294 - val_matthews_correlation: 0.6120\n",
      "\n",
      "Epoch 00015: val_matthews_correlation improved from 0.60493 to 0.61200, saving model to weights_4.h5\n",
      "Epoch 16/50\n",
      "2324/2324 [==============================] - 1s 506us/step - loss: 0.1096 - matthews_correlation: 0.5787 - val_loss: 0.1168 - val_matthews_correlation: 0.6470\n",
      "\n",
      "Epoch 00016: val_matthews_correlation improved from 0.61200 to 0.64702, saving model to weights_4.h5\n",
      "Epoch 17/50\n",
      "2324/2324 [==============================] - 1s 508us/step - loss: 0.1021 - matthews_correlation: 0.6329 - val_loss: 0.1211 - val_matthews_correlation: 0.6375\n",
      "\n",
      "Epoch 00017: val_matthews_correlation did not improve from 0.64702\n",
      "Epoch 18/50\n",
      "2324/2324 [==============================] - 1s 505us/step - loss: 0.1037 - matthews_correlation: 0.6122 - val_loss: 0.1181 - val_matthews_correlation: 0.6005\n",
      "\n",
      "Epoch 00018: val_matthews_correlation did not improve from 0.64702\n",
      "Epoch 19/50\n",
      "2324/2324 [==============================] - 1s 507us/step - loss: 0.0982 - matthews_correlation: 0.6762 - val_loss: 0.1153 - val_matthews_correlation: 0.6437\n",
      "\n",
      "Epoch 00019: val_matthews_correlation did not improve from 0.64702\n",
      "Epoch 20/50\n",
      "2324/2324 [==============================] - 1s 504us/step - loss: 0.1009 - matthews_correlation: 0.6492 - val_loss: 0.1316 - val_matthews_correlation: 0.5436\n",
      "\n",
      "Epoch 00020: val_matthews_correlation did not improve from 0.64702\n",
      "Epoch 21/50\n",
      "2324/2324 [==============================] - 1s 507us/step - loss: 0.0960 - matthews_correlation: 0.6726 - val_loss: 0.1257 - val_matthews_correlation: 0.3862\n",
      "\n",
      "Epoch 00021: val_matthews_correlation did not improve from 0.64702\n",
      "Epoch 22/50\n",
      "2324/2324 [==============================] - 1s 508us/step - loss: 0.0935 - matthews_correlation: 0.6573 - val_loss: 0.1207 - val_matthews_correlation: 0.4110\n",
      "\n",
      "Epoch 00022: val_matthews_correlation did not improve from 0.64702\n",
      "Epoch 23/50\n",
      "2324/2324 [==============================] - 1s 507us/step - loss: 0.0937 - matthews_correlation: 0.6338 - val_loss: 0.1154 - val_matthews_correlation: 0.6354\n",
      "\n",
      "Epoch 00023: val_matthews_correlation did not improve from 0.64702\n",
      "Epoch 24/50\n",
      "2324/2324 [==============================] - 1s 504us/step - loss: 0.0982 - matthews_correlation: 0.6840 - val_loss: 0.1337 - val_matthews_correlation: 0.3895\n",
      "\n",
      "Epoch 00024: val_matthews_correlation did not improve from 0.64702\n",
      "Epoch 25/50\n",
      "2324/2324 [==============================] - 1s 506us/step - loss: 0.0981 - matthews_correlation: 0.6296 - val_loss: 0.1118 - val_matthews_correlation: 0.6449\n",
      "\n",
      "Epoch 00025: val_matthews_correlation did not improve from 0.64702\n",
      "Epoch 26/50\n",
      "2324/2324 [==============================] - 1s 508us/step - loss: 0.0923 - matthews_correlation: 0.6980 - val_loss: 0.1250 - val_matthews_correlation: 0.5928\n",
      "\n",
      "Epoch 00026: val_matthews_correlation did not improve from 0.64702\n",
      "Epoch 27/50\n",
      "2324/2324 [==============================] - 1s 520us/step - loss: 0.0998 - matthews_correlation: 0.6487 - val_loss: 0.1237 - val_matthews_correlation: 0.6436\n",
      "\n",
      "Epoch 00027: val_matthews_correlation did not improve from 0.64702\n",
      "Epoch 28/50\n",
      "2324/2324 [==============================] - 1s 492us/step - loss: 0.0991 - matthews_correlation: 0.6673 - val_loss: 0.1196 - val_matthews_correlation: 0.4101\n",
      "\n",
      "Epoch 00028: val_matthews_correlation did not improve from 0.64702\n",
      "Epoch 29/50\n",
      "2324/2324 [==============================] - 1s 508us/step - loss: 0.0920 - matthews_correlation: 0.6810 - val_loss: 0.1168 - val_matthews_correlation: 0.6115\n",
      "\n",
      "Epoch 00029: val_matthews_correlation did not improve from 0.64702\n",
      "Epoch 30/50\n",
      "2324/2324 [==============================] - 1s 506us/step - loss: 0.0908 - matthews_correlation: 0.6735 - val_loss: 0.1240 - val_matthews_correlation: 0.5570\n",
      "\n",
      "Epoch 00030: val_matthews_correlation did not improve from 0.64702\n",
      "Epoch 31/50\n",
      "2324/2324 [==============================] - 1s 507us/step - loss: 0.0981 - matthews_correlation: 0.6777 - val_loss: 0.1159 - val_matthews_correlation: 0.6033\n",
      "\n",
      "Epoch 00031: val_matthews_correlation did not improve from 0.64702\n",
      "Epoch 32/50\n",
      "2324/2324 [==============================] - 1s 507us/step - loss: 0.0947 - matthews_correlation: 0.6273 - val_loss: 0.1180 - val_matthews_correlation: 0.5990\n",
      "\n",
      "Epoch 00032: val_matthews_correlation did not improve from 0.64702\n",
      "Epoch 33/50\n",
      "2324/2324 [==============================] - 1s 508us/step - loss: 0.0946 - matthews_correlation: 0.6816 - val_loss: 0.1231 - val_matthews_correlation: 0.3641\n",
      "\n",
      "Epoch 00033: val_matthews_correlation did not improve from 0.64702\n",
      "Epoch 34/50\n",
      "2324/2324 [==============================] - 1s 505us/step - loss: 0.0910 - matthews_correlation: 0.6899 - val_loss: 0.1149 - val_matthews_correlation: 0.6320\n",
      "\n",
      "Epoch 00034: val_matthews_correlation did not improve from 0.64702\n",
      "Epoch 35/50\n",
      "2324/2324 [==============================] - 1s 512us/step - loss: 0.0875 - matthews_correlation: 0.7104 - val_loss: 0.1182 - val_matthews_correlation: 0.4225\n",
      "\n",
      "Epoch 00035: val_matthews_correlation did not improve from 0.64702\n",
      "Epoch 36/50\n",
      "2324/2324 [==============================] - 1s 506us/step - loss: 0.0932 - matthews_correlation: 0.6770 - val_loss: 0.1146 - val_matthews_correlation: 0.6088\n",
      "\n",
      "Epoch 00036: val_matthews_correlation did not improve from 0.64702\n",
      "Epoch 37/50\n",
      "2324/2324 [==============================] - 1s 518us/step - loss: 0.0879 - matthews_correlation: 0.7084 - val_loss: 0.1118 - val_matthews_correlation: 0.5873\n",
      "\n",
      "Epoch 00037: val_matthews_correlation did not improve from 0.64702\n",
      "Epoch 38/50\n",
      "2324/2324 [==============================] - 1s 504us/step - loss: 0.0815 - matthews_correlation: 0.7083 - val_loss: 0.1238 - val_matthews_correlation: 0.5029\n",
      "\n",
      "Epoch 00038: val_matthews_correlation did not improve from 0.64702\n",
      "Epoch 39/50\n",
      "2324/2324 [==============================] - 1s 505us/step - loss: 0.0837 - matthews_correlation: 0.6873 - val_loss: 0.1194 - val_matthews_correlation: 0.5423\n",
      "\n",
      "Epoch 00039: val_matthews_correlation did not improve from 0.64702\n",
      "Epoch 40/50\n",
      "2324/2324 [==============================] - 1s 508us/step - loss: 0.0818 - matthews_correlation: 0.7071 - val_loss: 0.1096 - val_matthews_correlation: 0.6340\n",
      "\n",
      "Epoch 00040: val_matthews_correlation did not improve from 0.64702\n",
      "Epoch 41/50\n",
      "2324/2324 [==============================] - 1s 506us/step - loss: 0.0863 - matthews_correlation: 0.6945 - val_loss: 0.1171 - val_matthews_correlation: 0.5029\n",
      "\n",
      "Epoch 00041: val_matthews_correlation did not improve from 0.64702\n",
      "Epoch 42/50\n",
      "2324/2324 [==============================] - 1s 504us/step - loss: 0.0960 - matthews_correlation: 0.6538 - val_loss: 0.1104 - val_matthews_correlation: 0.6101\n",
      "\n",
      "Epoch 00042: val_matthews_correlation did not improve from 0.64702\n",
      "Epoch 43/50\n",
      "2324/2324 [==============================] - 1s 502us/step - loss: 0.0869 - matthews_correlation: 0.6976 - val_loss: 0.1123 - val_matthews_correlation: 0.6125\n",
      "\n",
      "Epoch 00043: val_matthews_correlation did not improve from 0.64702\n",
      "Epoch 44/50\n",
      "2324/2324 [==============================] - 1s 505us/step - loss: 0.0773 - matthews_correlation: 0.7031 - val_loss: 0.1300 - val_matthews_correlation: 0.6133\n",
      "\n",
      "Epoch 00044: val_matthews_correlation did not improve from 0.64702\n",
      "Epoch 45/50\n",
      "2324/2324 [==============================] - 1s 508us/step - loss: 0.0763 - matthews_correlation: 0.7324 - val_loss: 0.1140 - val_matthews_correlation: 0.5161\n",
      "\n",
      "Epoch 00045: val_matthews_correlation did not improve from 0.64702\n",
      "Epoch 46/50\n",
      "2324/2324 [==============================] - 1s 512us/step - loss: 0.0809 - matthews_correlation: 0.6617 - val_loss: 0.1266 - val_matthews_correlation: 0.4214\n",
      "\n",
      "Epoch 00046: val_matthews_correlation did not improve from 0.64702\n",
      "Epoch 47/50\n",
      "2324/2324 [==============================] - 1s 505us/step - loss: 0.0783 - matthews_correlation: 0.7011 - val_loss: 0.1241 - val_matthews_correlation: 0.6116\n",
      "\n",
      "Epoch 00047: val_matthews_correlation did not improve from 0.64702\n",
      "Epoch 48/50\n",
      "2324/2324 [==============================] - 1s 507us/step - loss: 0.0844 - matthews_correlation: 0.7411 - val_loss: 0.1267 - val_matthews_correlation: 0.5108\n",
      "\n",
      "Epoch 00048: val_matthews_correlation did not improve from 0.64702\n",
      "Epoch 49/50\n",
      "2324/2324 [==============================] - 1s 504us/step - loss: 0.0804 - matthews_correlation: 0.7233 - val_loss: 0.1161 - val_matthews_correlation: 0.6458\n",
      "\n",
      "Epoch 00049: val_matthews_correlation did not improve from 0.64702\n",
      "Epoch 50/50\n",
      "2324/2324 [==============================] - 1s 516us/step - loss: 0.0814 - matthews_correlation: 0.7186 - val_loss: 0.1245 - val_matthews_correlation: 0.5513\n",
      "\n",
      "Epoch 00050: val_matthews_correlation did not improve from 0.64702\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((2904,), (2904,))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here is where the training happens\n",
    "\n",
    "# First, create a set of indexes of the 5 folds\n",
    "splits = list(StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=2019).split(X, y))\n",
    "preds_val = []\n",
    "y_val = []\n",
    "# Then, iteract with each fold\n",
    "# If you dont know, enumerate(['a', 'b', 'c']) returns [(0, 'a'), (1, 'b'), (2, 'c')]\n",
    "for idx, (train_idx, val_idx) in enumerate(splits):\n",
    "    K.clear_session() # I dont know what it do, but I imagine that it \"clear session\" :)\n",
    "    print(\"Beginning fold {}\".format(idx+1))\n",
    "    # use the indexes to extract the folds in the train and validation data\n",
    "    train_X, train_y, val_X, val_y = X[train_idx], y[train_idx], X[val_idx], y[val_idx]\n",
    "    # instantiate the model for this fold\n",
    "    model = model_lstm(train_X.shape)\n",
    "    # This checkpoint helps to avoid overfitting. It just save the weights of the model if it delivered an\n",
    "    # validation matthews_correlation greater than the last one.\n",
    "    ckpt = ModelCheckpoint('weights_{}.h5'.format(idx), save_best_only=True, save_weights_only=True, verbose=1, monitor='val_matthews_correlation', mode='max')\n",
    "    # Train, train, train\n",
    "    history=model.fit(train_X, train_y, batch_size=128, epochs=50, validation_data=[val_X, val_y], callbacks=[ckpt])\n",
    "    # loads the best weights saved by the checkpoint\n",
    "    model.load_weights('weights_{}.h5'.format(idx))\n",
    "    # Add the predictions of the validation to the list preds_val\n",
    "    preds_val.append(model.predict(val_X, batch_size=512))\n",
    "    # and the val true y\n",
    "    y_val.append(val_y)\n",
    "\n",
    "# concatenates all and prints the shape    \n",
    "preds_val = np.concatenate(preds_val)[...,0]\n",
    "y_val = np.concatenate(y_val)\n",
    "preds_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history[\"matthews_correlation\"])\n",
    "plt.plot(history.history[\"val_matthews_correlation\"])\n",
    "plt.title(\"model accuracy\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend([\"Train\",\"test\"], loc=\"upper left\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    "plt.title(\"model loss\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend([\"train\",\"test\"],loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "d28151fd0be9fd9762f3f55e307d82f89bfbd291"
   },
   "outputs": [],
   "source": [
    "# The output of this kernel must be binary (0 or 1), but the output of the NN Model is float (0 to 1).\n",
    "# So, find the best threshold to convert float to binary is crucial to the result\n",
    "# this piece of code is a function that evaluates all the possible thresholds from 0 to 1 by 0.01\n",
    "def threshold_search(y_true, y_proba):\n",
    "    best_threshold = 0\n",
    "    best_score = 0\n",
    "    for threshold in tqdm([i * 0.01 for i in range(100)]):\n",
    "        score = K.eval(matthews_correlation(y_true.astype(np.float64), (y_proba > threshold).astype(np.float64)))\n",
    "        if score > best_score:\n",
    "            best_threshold = threshold\n",
    "            best_score = score\n",
    "    search_result = {'threshold': best_threshold, 'matthews_correlation': best_score}\n",
    "    return search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "6fee7f722ed08bc1453a822a4371ed2d48e08abc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:20<00:00,  3.28it/s]\n"
     ]
    }
   ],
   "source": [
    "best_threshold = threshold_search(y_val, preds_val)['threshold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "ae9bd3fa9d8c0781c0708846bb7f2a9f9e6cbd3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.4 ms, sys: 2.89 ms, total: 13.3 ms\n",
      "Wall time: 28.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Now load the test data\n",
    "# This first part is the meta data, not the main data, the measurements\n",
    "meta_test = pd.read_csv('../input/metadata_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "3eb186d032f79c99ffba05dd1a7fabb77e13cec5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_measurement</th>\n",
       "      <th>phase</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>signal_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8712</th>\n",
       "      <td>2904</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8713</th>\n",
       "      <td>2904</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8714</th>\n",
       "      <td>2904</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8715</th>\n",
       "      <td>2905</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8716</th>\n",
       "      <td>2905</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id_measurement  phase\n",
       "signal_id                       \n",
       "8712                 2904      0\n",
       "8713                 2904      1\n",
       "8714                 2904      2\n",
       "8715                 2905      0\n",
       "8716                 2905      1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_test = meta_test.set_index(['signal_id'])\n",
    "meta_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "6f8e94387f625bff0a9a6289e1ee038908bc5856"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8712 10 20337 2033 7 20337\n",
      "[[8712, 10745], [10745, 12778], [12778, 14811], [14811, 16844], [16844, 18877], [18877, 20910], [20910, 22943], [22943, 24976], [24976, 27009], [27009, 29042], [29042, 29049]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2033/2033 [03:30<00:00,  9.67it/s]\n",
      "100%|██████████| 2033/2033 [03:31<00:00,  9.75it/s]\n",
      "100%|██████████| 2033/2033 [03:30<00:00,  9.65it/s]\n",
      "100%|██████████| 2033/2033 [03:30<00:00, 10.03it/s]\n",
      "100%|██████████| 2033/2033 [03:29<00:00,  9.97it/s]\n",
      "100%|██████████| 2033/2033 [03:30<00:00,  7.29it/s]\n",
      "100%|██████████| 2033/2033 [03:30<00:00, 10.09it/s]\n",
      "100%|██████████| 2033/2033 [03:29<00:00, 10.17it/s]\n",
      "100%|██████████| 2033/2033 [03:29<00:00,  7.92it/s]\n",
      "100%|██████████| 2033/2033 [03:28<00:00, 10.04it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  8.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36min 43s, sys: 34.4 s, total: 37min 17s\n",
      "Wall time: 38min 53s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# First we daclarete a series of parameters to initiate the loading of the main data\n",
    "# it is too large, it is impossible to load in one time, so we are doing it in dividing in 10 parts\n",
    "first_sig = meta_test.index[0]\n",
    "n_parts = 10\n",
    "max_line = len(meta_test)\n",
    "part_size = int(max_line / n_parts)\n",
    "last_part = max_line % n_parts\n",
    "print(first_sig, n_parts, max_line, part_size, last_part, n_parts * part_size + last_part)\n",
    "# Here we create a list of lists with start index and end index for each of the 10 parts and one for the last partial part\n",
    "start_end = [[x, x+part_size] for x in range(first_sig, max_line + first_sig, part_size)]\n",
    "start_end = start_end[:-1] + [[start_end[-1][0], start_end[-1][0] + last_part]]\n",
    "print(start_end)\n",
    "X_test = []\n",
    "# now, very like we did above with the train data, we convert the test data part by part\n",
    "# transforming the 3 phases 800000 measurement in matrix (160,57)\n",
    "for start, end in start_end:\n",
    "    subset_test = pq.read_pandas('../input/test.parquet', columns=[str(i) for i in range(start, end)]).to_pandas()\n",
    "    for i in tqdm(subset_test.columns):\n",
    "        id_measurement, phase = meta_test.loc[int(i)]\n",
    "        subset_test_col = subset_test[i]\n",
    "        subset_trans = transform_ts(subset_test_col)\n",
    "        X_test.append([i, id_measurement, phase, subset_trans])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model,to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "af9aa6b2b8f8a2beda1a02ff998e3072fcad8d06"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6779, 160, 57)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_input = np.asarray([np.concatenate([X_test[i][3],X_test[i+1][3], X_test[i+2][3]], axis=1) for i in range(0,len(X_test), 3)])\n",
    "np.save(\"X_test.npy\",X_test_input)\n",
    "X_test_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "cfd265d3e07c4cc1679d2c4d55fe7de631c813e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20337\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>signal_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8712</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8713</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8714</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8715</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8716</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   signal_id  target\n",
       "0       8712       0\n",
       "1       8713       0\n",
       "2       8714       0\n",
       "3       8715       0\n",
       "4       8716       0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.read_csv('../input/sample_submission.csv')\n",
    "print(len(submission))\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "2f7342296138f6bfd3e9cedd029e1035de3b98fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6779/6779 [==============================] - 1s 135us/step\n",
      "6779/6779 [==============================] - 1s 134us/step\n",
      "6779/6779 [==============================] - 1s 130us/step\n",
      "6779/6779 [==============================] - 1s 130us/step\n",
      "6779/6779 [==============================] - 1s 131us/step\n"
     ]
    }
   ],
   "source": [
    "preds_test = []\n",
    "for i in range(N_SPLITS):\n",
    "    model.load_weights('weights_{}.h5'.format(i))\n",
    "    pred = model.predict(X_test_input, batch_size=300, verbose=1)\n",
    "    pred_3 = []\n",
    "    for pred_scalar in pred:\n",
    "        for i in range(3):\n",
    "            pred_3.append(pred_scalar)\n",
    "    preds_test.append(pred_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "9f76c471eaf983707d446c5081ab3d50c4e40ea5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20337,)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_test = (np.squeeze(np.mean(preds_test, axis=0)) > best_threshold).astype(np.int)\n",
    "preds_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "b35723f85d494b4b6ec630dd7c79135a110a4062"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>signal_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8712</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8713</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8714</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8715</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8716</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   signal_id  target\n",
       "0       8712       0\n",
       "1       8713       0\n",
       "2       8714       0\n",
       "3       8715       0\n",
       "4       8716       0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission['target'] = preds_test\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "d7600d0093a9880003240ef9ce0a1f1303e4d982"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
